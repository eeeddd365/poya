import os
from playwright.sync_api import sync_playwright
from supabase import create_client

# ç’°å¢ƒè®Šæ•¸
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# æ¸¬è©¦ç‰¹å®šç¶²å€
TARGET_URL = "https://www.poyabuy.com.tw/v2/official/SalePageCategory/374016?sortMode=Sales"

def scrape():
    with sync_playwright() as p:
        # å•Ÿå‹•ç€è¦½å™¨
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 800}
        )
        page = context.new_page()

        print(f"ğŸš€ æ­£åœ¨å­˜å–ç¶²å€: {TARGET_URL}")
        
        try:
            # é€²å…¥é é¢ä¸¦ç­‰å¾…
            page.goto(TARGET_URL, wait_until="networkidle", timeout=60000)
            page.wait_for_timeout(7000) # å¤šç­‰ 7 ç§’ç¢ºä¿æ¸²æŸ“å®Œæˆ

            # æ¨¡æ“¬æ²å‹•è§¸ç™¼ Lazy Load
            page.mouse.wheel(0, 2000)
            page.wait_for_timeout(3000)

            # åµéŒ¯ï¼šæˆªåœ–ç•™å­˜ï¼ˆå¦‚æœå¤±æ•—å¯ä»¥åœ¨ GitHub Actions çœ‹åˆ°é é¢é•·æ€æ¨£ï¼‰
            page.screenshot(path="debug_screen.png")
            print("ğŸ“¸ å·²æˆªåœ–å­˜æª”ç‚º debug_screen.png")

            # æŠ“å–é‚è¼¯ï¼šé‡å° 91APP é«”ç³»çš„å•†å“å¡ç‰‡çµæ§‹
            # 1. å…ˆæŠ“æ‰€æœ‰å•†å“ A æ¨™ç±¤
            product_nodes = page.locator("a[href*='SalePage']").all()
            print(f"ğŸ” æ‰¾åˆ°æ½›åœ¨å•†å“ç¯€é»æ•¸: {len(product_nodes)}")

            data_list = []
            for node in product_nodes:
                try:
                    # æŠ“å–åœ–ç‰‡ï¼šå°‹æ‰¾ A æ¨™ç±¤å…§çš„ img
                    img_el = node.locator("img").first
                    img_url = img_el.get_attribute("src") or img_el.get_attribute("data-src")
                    
                    # æŠ“å–æ¨™é¡Œï¼šå°‹æ‰¾åŒ…å«æ¨™é¡Œæ–‡å­—çš„ div æˆ– p
                    title = node.inner_text().split('\n')[0].strip()

                    if title and img_url and len(title) > 5:
                        if img_url.startswith("//"):
                            img_url = "https:" + img_url
                        
                        data_list.append({
                            "title": title,
                            "image_url": img_url,
                            "category": "ç´™æ£‰ç”¨å“"
                        })
                except:
                    continue

            # å¯«å…¥è³‡æ–™åº«
            if data_list:
                # ç°¡å–®å»é‡
                unique_data = {v['title']: v for v in data_list}.values()
                print(f"ğŸ’¾ æº–å‚™å¯«å…¥ {len(unique_data)} ç­†å•†å“åˆ° Supabase...")
                supabase.table("poya_items").upsert(list(unique_data), on_conflict="title").execute()
                print("âœ… å¯«å…¥æˆåŠŸï¼")
            else:
                print("âŒ ä¾ç„¶æ²’æŠ“åˆ°å•†å“ã€‚è«‹ç¢ºèª Table 'poya_items' çš„ title æ¬„ä½æ˜¯å¦è¨­ç‚º Primary Keyã€‚")

        except Exception as e:
            print(f"âŒ åŸ·è¡Œå‡ºéŒ¯: {e}")
        
        browser.close()

if __name__ == "__main__":
    scrape()
