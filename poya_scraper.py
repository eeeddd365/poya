import os
import time
from playwright.sync_api import sync_playwright
from supabase import create_client

# ç’°å¢ƒè®Šæ•¸
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# å¯¶é›…åˆ†é¡ ID
CATEGORIES = {
    "ç´™æ£‰ç”¨å“": "260",
    "å±…å®¶æ¸…æ½”": "261",
    "ç”Ÿæ´»é›œè²¨": "262",
    "ç”Ÿæ´»ç”¨å“": "263"
}

def scrape_poya():
    with sync_playwright() as p:
        # å•Ÿå‹•æ™‚åŠ å…¥æ›´å¤šå½è£åƒæ•¸
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080},
            extra_http_headers={
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
            }
        )
        page = context.new_page()

        for cat_name, cat_id in CATEGORIES.items():
            url = f"https://www.poyabuy.com.tw/v2/official/SalePageCategory/{cat_id}"
            print(f"ğŸš€ æ­£åœ¨é€²å…¥åˆ†é¡: {cat_name}...")
            
            try:
                # é€²å…¥é é¢ï¼Œä¸¦å¤šç­‰å¹¾ç§’è®“ JavaScript è·‘å®Œ
                page.goto(url, wait_until="networkidle", timeout=60000)
                page.wait_for_timeout(8000) 

                # æ¨¡æ“¬äººé¡å‘ä¸‹æ»¾å‹•ï¼Œé€™å°è§¸ç™¼ Lazy Load åœ–ç‰‡å¾ˆé‡è¦
                page.mouse.wheel(0, 1500)
                page.wait_for_timeout(2000)
                page.mouse.wheel(0, 1500)
                page.wait_for_timeout(2000)

                # ã€æ ¸å¿ƒæ”¹å‹•ã€‘ä½¿ç”¨æ›´å»£æ³›çš„é¸å–å™¨ï¼ŒæŠ“å–æ‰€æœ‰çœ‹èµ·ä¾†åƒå•†å“çš„ A é€£çµ
                # å¯¶é›…çš„å•†å“é€£çµé€šå¸¸åŒ…å« 'SalePage'
                product_links = page.locator("a[href*='SalePage']").all()
                print(f"ğŸ” ç¶²é ä¸­åµæ¸¬åˆ° {len(product_links)} å€‹å•†å“é€£çµ...")

                data_list = []
                for link in product_links:
                    try:
                        # æŠ“å–é€£çµå…§çš„æ–‡å­—ä½œç‚ºæ¨™é¡Œ
                        title = link.inner_text().split('\n')[0].strip()
                        # æŠ“å–é€£çµå…§çš„ç¬¬ä¸€å¼µåœ–ç‰‡
                        img_element = link.locator("img").first
                        img_url = img_element.get_attribute("src") or img_element.get_attribute("data-src")

                        if title and img_url and len(title) > 2:
                            # æ ¼å¼åŒ–åœ–ç‰‡ URL
                            if img_url.startswith("//"):
                                img_url = "https:" + img_url
                            
                            # æ’é™¤æ‰å»£å‘Šæˆ–å°çš„ icon (é€šå¸¸å°æ–¼ 50 å­—å…ƒçš„ç¶²å€å¯èƒ½ä¸æ˜¯å•†å“åœ–)
                            if "static" not in img_url:
                                data_list.append({
                                    "title": title,
                                    "image_url": img_url,
                                    "category": cat_name
                                })
                    except:
                        continue
                
                # ç§»é™¤é‡è¤‡çš„æ¨™é¡Œ
                if data_list:
                    unique_data = {v['title']: v for v in data_list}.values()
                    print(f"ğŸ’¾ æˆåŠŸéæ¿¾å‡º {len(unique_data)} ç­†æœ‰æ•ˆå•†å“ï¼Œå¯«å…¥ Supabase...")
                    supabase.table("poya_items").upsert(list(unique_data), on_conflict="title").execute()
                else:
                    print(f"âŒ ç„¡æ³•æŠ“å–åˆ°å•†å“å…§å®¹ï¼Œè«‹æª¢æŸ¥ç¶²ç«™æ˜¯å¦å°é–äº† IPã€‚")

            except Exception as e:
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

        browser.close()

if __name__ == "__main__":
    scrape_poya()
